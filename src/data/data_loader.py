import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
import os
import tarfile
import urllib.request  # ç”¨äºŽä¸‹è½½æ–‡ä»¶
from tqdm import tqdm  # æ˜¾ç¤ºä¸‹è½½è¿›åº¦ï¼ˆéœ€å®‰è£…ï¼špip install tqdmï¼‰

# 1. æ•°æ®é›†ä¿å­˜è·¯å¾„ï¼ˆå’Œä½ ä¹‹å‰çš„è·¯å¾„ä¸€è‡´ï¼‰
TEXT_SAVE_DIR = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
    "data",
    "multi30k-de-en"
)
os.makedirs(TEXT_SAVE_DIR, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨

# 2. MULTI30K æ•°æ®é›†å®˜æ–¹ä¸‹è½½é“¾æŽ¥ï¼ˆGitHubé•œåƒï¼Œé¿å…åŽŸURLå¤±æ•ˆï¼‰
# åŒ…å«ï¼šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†ï¼ˆDEâ†’ENï¼‰
URLS = {
    "train": "https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz",
    "val": "https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz",
    "test": "https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz"
}


def auto_download_multi30k():
    """è‡ªåŠ¨ä¸‹è½½å¹¶è§£åŽ‹ MULTI30K æ•°æ®é›†åˆ° TEXT_SAVE_DIR"""
    # éœ€ä¸‹è½½çš„æ–‡ä»¶ï¼štrain.de/train.enã€val.de/val.enï¼ˆæµ‹è¯•é›†å¯é€‰ï¼‰
    required_files = [
        os.path.join(TEXT_SAVE_DIR, "train.de"),
        os.path.join(TEXT_SAVE_DIR, "train.en"),
        os.path.join(TEXT_SAVE_DIR, "val.de"),
        os.path.join(TEXT_SAVE_DIR, "val.en")
    ]

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤ä¸‹è½½
    if all(os.path.exists(f) for f in required_files):
        print(f"âœ… MULTI30K æ•°æ®é›†å·²å­˜åœ¨ï¼Œæ— éœ€ä¸‹è½½")
        return

    # 3. ä¸‹è½½å¹¶è§£åŽ‹æ¯ä¸ªæ–‡ä»¶
    for split in ["train", "val"]:  # å…ˆä¸‹è½½è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆæµ‹è¯•é›†å¯é€‰ï¼‰
        url = URLS[split]
        tar_path = os.path.join(TEXT_SAVE_DIR, f"{split}.tar.gz")  # ä¸´æ—¶ä¿å­˜åŽ‹ç¼©åŒ…

        # ä¸‹è½½åŽ‹ç¼©åŒ…ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
        print(f"ðŸ“¥ ä¸‹è½½ {split} é›†ï¼š{url}")
        with tqdm(unit="B", unit_scale=True, miniters=1, desc=split) as t:
            def update_progress(block_num, block_size, total_size):
                t.total = total_size
                t.update(block_num * block_size - t.n)

            urllib.request.urlretrieve(url, tar_path, reporthook=update_progress)

        # è§£åŽ‹å¹¶æå–æ‰€éœ€æ–‡ä»¶ï¼ˆåªä¿ç•™ DE å’Œ EN æ–‡æœ¬ï¼‰
        print(f"ðŸ“¦ è§£åŽ‹ {split} é›†åˆ° {TEXT_SAVE_DIR}...")
        with tarfile.open(tar_path, "r:gz") as tar:
            for member in tar.getmembers():
                # åŒ¹é…å¾·æ–‡ï¼ˆ.deï¼‰å’Œè‹±æ–‡ï¼ˆ.enï¼‰æ–‡ä»¶
                if member.name.endswith(".de") or member.name.endswith(".en"):
                    # éªŒè¯é›†åŽŸæ–‡ä»¶å« "valid.de/en"ï¼Œéœ€é‡å‘½åä¸º "val.de/en"ï¼ˆåŒ¹é…ä½ çš„ä»£ç é€»è¾‘ï¼‰
                    if split == "val" and "valid" in member.name:
                        new_name = member.name.replace("valid", "val")
                        member.name = new_name
                    # è§£åŽ‹åˆ°ç›®æ ‡ç›®å½•
                    tar.extract(member, path=TEXT_SAVE_DIR)

        # åˆ é™¤ä¸´æ—¶åŽ‹ç¼©åŒ…
        os.remove(tar_path)

    print(f"âœ… æ‰€æœ‰æ•°æ®é›†ä¸‹è½½å®Œæˆï¼Œä¿å­˜è·¯å¾„ï¼š{TEXT_SAVE_DIR}")


# è‡ªåŠ¨æ‰§è¡Œä¸‹è½½ï¼ˆè¿è¡Œä»£ç æ—¶è§¦å‘ï¼Œæ— éœ€æ‰‹åŠ¨æ“ä½œï¼‰
auto_download_multi30k()


class Multi30kDataset(Dataset):
    def __init__(self, split: str = "train", max_seq_len: int = 64):
        super().__init__()
        self.max_seq_len = max_seq_len
        self.src_lang = "de"  # å¾·æ–‡â†’è‹±æ–‡
        self.tgt_lang = "en"
        self.tokenizer = AutoTokenizer.from_pretrained(
            "t5-small",
            src_lang=self.src_lang,
            tgt_lang=self.tgt_lang
        )

        # è¯»å–è‡ªåŠ¨ä¸‹è½½çš„æ–‡ä»¶ï¼ˆè·¯å¾„åŒ¹é… auto_download_multi30k ç”Ÿæˆçš„æ–‡ä»¶ï¼‰
        self.src_path = os.path.join(TEXT_SAVE_DIR, f"{split}.{self.src_lang}")
        self.tgt_path = os.path.join(TEXT_SAVE_DIR, f"{split}.{self.tgt_lang}")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        self._check_file_exists()

        # è¯»å–æ•°æ®
        with open(self.src_path, "r", encoding="utf-8") as f:
            self.src_texts = [line.strip() for line in f if line.strip()]
        with open(self.tgt_path, "r", encoding="utf-8") as f:
            self.tgt_texts = [line.strip() for line in f if line.strip()]

        # éªŒè¯å¥å¯¹æ•°é‡
        assert len(self.src_texts) == len(self.tgt_texts), \
            f"âŒ {split}é›† {self.src_lang} å’Œ {self.tgt_lang} æ•°é‡ä¸åŒ¹é…ï¼"
        print(f"âœ… åŠ è½½ {split}é›†ï¼š{len(self.src_texts)} æ¡ {self.src_lang}â†’{self.tgt_lang} å¥å¯¹")

    def _check_file_exists(self):
        missing_files = []
        if not os.path.exists(self.src_path):
            missing_files.append(self.src_path)
        if not os.path.exists(self.tgt_path):
            missing_files.append(self.tgt_path)
        if missing_files:
            raise FileNotFoundError(
                f"âŒ ç¼ºå¤±æ–‡ä»¶ï¼ˆè¯·ç¡®ä¿ä¸‹è½½æˆåŠŸï¼‰ï¼š\n"
                + "\n".join(missing_files)
            )

    def __len__(self) -> int:
        return len(self.src_texts)

    def __getitem__(self, idx: int) -> dict:
        src_text = self.src_texts[idx]
        tgt_text = self.tgt_texts[idx]

        src_encodings = self.tokenizer(
            src_text,
            max_length=self.max_seq_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        tgt_encodings = self.tokenizer(
            tgt_text,
            max_length=self.max_seq_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        return {
            "src_ids": src_encodings["input_ids"].squeeze(0),
            "src_mask": src_encodings["attention_mask"].squeeze(0),
            "tgt_ids": tgt_encodings["input_ids"].squeeze(0),
            "tgt_mask": tgt_encodings["attention_mask"].squeeze(0)
        }


def get_multi30k_dataloader(
        split: str = "train",
        max_seq_len: int = 64,
        batch_size: int = 32,
        shuffle: bool = True
) -> tuple[DataLoader, int]:
    dataset = Multi30kDataset(split=split, max_seq_len=max_seq_len)
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=True, num_workers=0
    )
    return dataloader, dataset.tokenizer.vocab_size